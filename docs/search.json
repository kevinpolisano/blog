[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Kévin Polisano",
    "section": "",
    "text": "Originaire de Lorraine, je vis depuis 2010 au milieu des montages à Grenoble, où j’ai fait mes études d’ingénieur à l’Ensimag puis un doctorat au Laboratoire Jean Kuntzmann. Depuis 2019 je suis chargé de recherche au CNRS en mathématiques appliquées."
  },
  {
    "objectID": "about.html#qui-suis-je",
    "href": "about.html#qui-suis-je",
    "title": "Kévin Polisano",
    "section": "",
    "text": "Originaire de Lorraine, je vis depuis 2010 au milieu des montages à Grenoble, où j’ai fait mes études d’ingénieur à l’Ensimag puis un doctorat au Laboratoire Jean Kuntzmann. Depuis 2019 je suis chargé de recherche au CNRS en mathématiques appliquées."
  },
  {
    "objectID": "about.html#pourquoi-ce-blog",
    "href": "about.html#pourquoi-ce-blog",
    "title": "Kévin Polisano",
    "section": "Pourquoi ce blog ?",
    "text": "Pourquoi ce blog ?\nCe blog est un support d’expérimentation et de partage de mes réflexions sur divers sujets."
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#plan-de-la-présentation",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#plan-de-la-présentation",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Plan de la présentation",
    "text": "Plan de la présentation\n\nIntroduction\nLes modèles linéaires\nGénéalogie des réseaux de neurones artificiels\nDeep learning – les réseaux de neurones convolutifs\nConclusion"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#définitions",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#définitions",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Définitions",
    "text": "Définitions\nIntelligence artificielle et apprentissage statistique\n\nIntelligence artificielle (IA) : ensemble de théories et de techniques visant à réaliser des machines capables de simuler l’intelligence humaine.\nApprentissage statistique ou Machine Learning (ML) : champ d’étude de l’intelligence artificielle qui se fonde sur des approches mathématiques et statistiques pour donner aux ordinateurs la capacité d’« apprendre » à partir de données, c’est-à-dire d’améliorer leurs performances à résoudre des tâches sans être explicitement programmés pour chacune.\n\n\nSource : Wikipédia\n\n\nl’Intelligence Artificielle (IA) est un ensemble de techniques permettant à des machines d’accomplir des tâches et de résoudre des problèmes normalement réservés aux humains et à certains animaux"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#panorama-de-lintelligence-artificielle",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#panorama-de-lintelligence-artificielle",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Panorama de l’intelligence artificielle",
    "text": "Panorama de l’intelligence artificielle\nLe machine learning est une branche de l’IA\n\n\nSource : inventiv-it.fr\n\n\nL’image illustre les relations entre l’Intelligence Artificielle (IA), le Machine Learning (apprentissage automatique), les réseaux neuronaux, et le Deep Learning (apprentissage profond), montrant comment ces concepts s’emboîtent et se distinguent.\nIntelligence Artificielle (IA) : C’est comme je le disais un domaine large qui vise à créer des systèmes capables d’exécuter des tâches nécessitant normalement l’intelligence humaine, telles que la reconnaissance vocale, la vision par ordinateur, le traitement automatique du langage naturel (NLP), la planification et la robotique. L’IA englobe divers sous-domaines, y compris le Machine Learning, et s’étend à des systèmes experts qui ne se basent pas toujours sur des méthodes d’apprentissage, mais sur des règles programmées. Dans cet exposé nous ne traiterons pas ce type d’intelligence artifielle. Nous nous focaliserons sur le :\nMachine Learning (Apprentissage automatique) : Il constitue une sous-partie de l’IA et se concentre sur le développement de modèles qui apprennent à partir de données. Contrairement aux approches traditionnelles de programmation, où des règles sont définies explicitement, le Machine Learning permet aux algorithmes de découvrir des modèles et de prendre des décisions à partir des données.\nNeural Networks (Réseaux de neurones) : C’est une technique clé dans le Machine Learning inspirée par la structure du cerveau humain. Un réseau de neurones est constitué de couches de nœuds (neurones) interconnectés qui permettent d’approximer des fonctions complexes.\nDeep Learning (Apprentissage profond) : des réseaux neuronaux profonds, c’est-à-dire qu’ils contiennent plusieurs couches de neurones. Les architectures de réseaux de neurones comme les réseaux convolutifs (CNN), les réseaux récurrents (RNN) et les réseaux adversaires génératifs (GAN) permettent de résoudre des tâches extrêmement complexes, notamment dans des domaines tels que la reconnaissance d’image, la traduction automatique, et les systèmes de recommandation. Le Deep Learning excelle là où les techniques traditionnelles de Machine Learning atteignent leurs limites, notamment dans le traitement de grandes quantités de données non structurées comme les images et les vidéos.\nEffectuons un zoom sur l’espace du Machine Learning."
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#la-carte-du-machine-learning",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#la-carte-du-machine-learning",
    "title": "Introduction à l’intelligence artificielle",
    "section": "La carte du Machine Learning",
    "text": "La carte du Machine Learning\n\n\nSource : Machine Learnia\n\n\nVoici un panorama du ML, partitionné en différentes composantes, des modèles paramétriques en orange, non-paramétriques en violet, des réseaux de neurones de bleu, etc. Evidemment en une heure c’est impossible de faire le tour de toutes les techniques de ML existantes, il y en a pléthore, donc je vous propose de visiter 2 familles emblématiques du ML paramétrique, à savoir les modèles linéaires – qui sont les précurseurs de tous les modèles d’apprentissage qui se sont développés depuis lors – et les CNN (les réseaux de neurones convolutifs) qui ont popularisés les méthodes dites de deep learning (ou apprentissage profond en français) qui ont révolutionné les techniques de classification d’images notamment."
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#la-régression-linéaire",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#la-régression-linéaire",
    "title": "Introduction à l’intelligence artificielle",
    "section": "La régression linéaire",
    "text": "La régression linéaire\nPrédiction du prix d’une maison en fonction de sa superficie\nDes données d’entrainement \\(\\color{red} (x_1,y_1),\\dotsc,(x_n,y_n)\\)\n\n\\(x_i\\) : superficie de la maison\n\\(y_i\\) : prix de la maison\n\\(n\\) : nombre de données\n\n\n\nSource : datahacker"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#la-régression-linéaire-1",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#la-régression-linéaire-1",
    "title": "Introduction à l’intelligence artificielle",
    "section": "La régression linéaire",
    "text": "La régression linéaire\n\nUn modèle de prédiction linéaire \\(\\color{blue} f_{w,b}\\)\nUne fonction de coût mesurant l’erreur (quadratique moyenne) : \\[ {\\color{red}J({\\color{blue}w},{\\color{blue}b})} = \\frac{1}{n} \\sum_{i=1}^n {\\color{red}\\ell({\\color{orange}y_i}, {\\color{blue}f_{w,b}\n({\\color{orange}x_i})})}, \\quad {\\color{red}\\ell(y,\\hat y)=(y-\\hat y)^2} \\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource : datahacker"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#la-régression-linéaire-2",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#la-régression-linéaire-2",
    "title": "Introduction à l’intelligence artificielle",
    "section": "La régression linéaire",
    "text": "La régression linéaire\n\nMinimisation du coût : \\(\\displaystyle \\color{red} (w^{\\star},b^{\\star})=\\underset{w,b}{\\mathrm{argmin}}\\; J(w,b)\\)\n\n\n\n\n\n\nSource : Geogebra"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#visualisation-de-la-descente-de-gradient",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#visualisation-de-la-descente-de-gradient",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Visualisation de la descente de gradient",
    "text": "Visualisation de la descente de gradient\nVideo\n\nSource : Son The Nguyen"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#la-régression-linéaire-3",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#la-régression-linéaire-3",
    "title": "Introduction à l’intelligence artificielle",
    "section": "La régression linéaire",
    "text": "La régression linéaire\nUne simple couche de neurone\n\n\nSource : Learn OpenCV"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#les-4-principaux-ingrédients-du-ml",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#les-4-principaux-ingrédients-du-ml",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Les 4 principaux ingrédients du ML",
    "text": "Les 4 principaux ingrédients du ML\n\nDes données\nUn modèle\nUne fonction de coût\nUn algorithme d’optimisation\n\n\n\nSource : Machine Learnia"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#la-régression-polynomiale",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#la-régression-polynomiale",
    "title": "Introduction à l’intelligence artificielle",
    "section": "La régression polynomiale",
    "text": "La régression polynomiale\nUn autre modèle linéaire plus complexe\n\n\nFitting des données bruitées \\({\\color{blue}y_i}={\\color{green}\\sin(2\\pi x_i)}+\\epsilon_i\\) avec un polynôme de degré \\(M\\) : \\[ \\color{red} f_{\\boldsymbol{w}}(x) = w_0 + w_1 x + w_2 x^2 + \\dotsc + w_M x^M = \\boldsymbol{\\psi}(x)^{\\intercal} \\boldsymbol{w}\\]\n\\(\\boldsymbol{\\psi}(x)=[1,x,x^2,\\dotsc,x^M]^{\\intercal}\\) est un feature mapping. On peut utiliser les moindres carrés puisque \\(f_{\\boldsymbol{w}}(x)=\\boldsymbol{\\psi}(x)^{\\intercal} \\boldsymbol{w}\\) est linéaire en \\(\\boldsymbol{w}\\).\n\n\nSource : (Bishop 2006)"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#la-régression-polynomiale-1",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#la-régression-polynomiale-1",
    "title": "Introduction à l’intelligence artificielle",
    "section": "La régression polynomiale",
    "text": "La régression polynomiale\nUnderfitting (M=0,1) vs. bon modèle (M=3) vs. overfitting (M=9)\n\n\nSource : (Bishop 2006)"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#la-régression-polynomiale-2",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#la-régression-polynomiale-2",
    "title": "Introduction à l’intelligence artificielle",
    "section": "La régression polynomiale",
    "text": "La régression polynomiale\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlus \\(M\\) augmente, plus les coefficients \\(w_i^{\\star}\\) explosent.\nLe polynôme \\(\\color{red} f_{\\boldsymbol{w}}(x)\\) finit par interpoler les données \\(\\color{blue} (x_i,y_i)\\) (erreur de training nulle pour \\(M=9\\)), avec de fortes oscillations entre ces points.\nLe modèle peine alors à généraliser sur les données de test (l’erreur grimpe pour \\(M=9\\)). Il faut ajuster l’hyperparamètre \\(M\\) sur données de validation (étape de sélection de modèle).\n\n\nSource : (Bishop 2006)"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#partitionnement-du-dataset",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#partitionnement-du-dataset",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Partitionnement du dataset",
    "text": "Partitionnement du dataset\nTraining vs. validation vs. testing\n\n\nSource : dhavalpatel"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#types-dapprentissages",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#types-dapprentissages",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Types d’apprentissages",
    "text": "Types d’apprentissages\nSupervisés, non-supervisés et par ré-enforcement\n\n\nSource : Techplayon\n\n\nIl inclut plusieurs techniques comme : Apprentissage supervisé : où un modèle est formé sur des données annotées (comme la régression linéaire/logistique, les arbres de décision, et les forêts aléatoires). Apprentissage non supervisé : où les modèles identifient des motifs dans des données non étiquetées. Apprentissage par renforcement : où un agent apprend à partir d’actions et de récompenses pour maximiser ses gains au fil du temps."
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#linvention-des-neurones-artificiels",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#linvention-des-neurones-artificiels",
    "title": "Introduction à l’intelligence artificielle",
    "section": "L’invention des neurones artificiels",
    "text": "L’invention des neurones artificiels\n(McCulloch and Pitts 1943)\n\n\nSource : Machine Learnia\n\n\nLes premiers réseaux de neurones ont donc été inventés en 1943 par deux mathématiciens et neuroscientifiques du nom de Warren McCulloch et Walter Pitts. Dans leur article scientifique intitulé : “A Logical Calculus of the ideas immanent in nervous activity”, ils expliquent comment ils ont pu programmer des neurones artificiels en s’inspirant du fonctionnement des neurones biologiques. Rappelons le, en biologie, les neurones sont des cellules excitables connectées les unes aux autres, et ayant pour rôle de transmettre des informations dans notre système nerveux. Chaque neurone est composé de plusieurs dendrites, d’un corps cellulaire, et d’un axone. Les dendrites sont en quelque sorte les portes d’entrée d’un neurone. c’est à cet endroit, au niveau de la synapse, que le neurone reçoit des signaux lui provenant des neurones qui le précèdent. Ces signaux peuvent être de type excitateur ou à l’inverse inhibiteur. (un peu comme si nous avions des signaux qui valent +1 et d’autres qui valent -1). Lorsque la somme de ces signaux dépasse un certain seuil, le neurone s’active et produit alors un signal électrique. Ce signal circule le long de l’axone en direction des terminaisons pour être envoyé à son tour vers d’autres neurones de notre système nerveux… …neurones qui fonctionneront exactement de la même manière ! Voilà en gros, le fonctionnement des neurones. Ce que Warren McCulloch et Walter Pitts ont essayé de faire, c’est de modéliser ce fonctionnement, en considérant qu’un neurone pouvait être représenté par une fonction de transfert, qui prend en entrée des signaux X et qui retourne une sortie y. A l’intérieur de cette fonction, on trouve 2 grandes étapes. La première, c’est une étape d’agrégation. On fait la somme de toutes les entrées du neurone, en multipliant au passage chaque entrée par un coefficient W. ce coefficient représente en fait l’activité synaptique, c’est à dire le fait que le signal soit excitateur auquel cas w vaut +1, ou bien inhibiteur auquel cas il vaut -1. Dans cette phase d’agrégation, on obtient donc une expression de la forme w1 x1 + w 2 x 2 + w3 x3 etc etc. Une fois cette étape réalisée, on passe à la phase d’activation. On regarde le résultat du calcul effectué précédemment, et si celui ci dépasse un certain seuil, en général 0, alors le neurone s’active et retourne une sortie y = 1. Sinon, il reste à 0. Voilà donc comment Warren McCulloch et Walter Pitts ont réussi à développer les premiers neurones artificiels"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#linvention-du-perceptron",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#linvention-du-perceptron",
    "title": "Introduction à l’intelligence artificielle",
    "section": "L’invention du Perceptron",
    "text": "L’invention du Perceptron\n(Rosenblatt 1958)\n\n\nSource : Machine Learnia\n\n\nLe modèle du Perceptron ressemble en fait de très près à celui que nous venons d’étudier. Il s’agit d’un neurone artificiel, qui s’active lorsque la somme pondérée de ses entrées dépasse un certain seuil, en général 0. Mais avec ça, le perceptron dispose également d’un algorithme d’apprentissage lui permettant de trouver les valeurs de ses paramètres w afin d’obtenir les sorties y qui nous conviennent. Pour développer cet algorithme, Frank Rosenblatt s’est inspiré de la théorie de Hebb. Cette théorie suggère que lorsque deux neurones biologiques sont excités conjointement, alors ils renforcent leurs liens synaptiques c’est-à-dire qu’ils renforcent les connexions qui les unissent. En neurosciences, c’est ce qu’on appelle la plasticité synaptique, et c’est ce qui permet à notre cerveau de construire sa mémoire, d’apprendre de nouvelles choses ou encore de faire de nouvelles associations. Donc, à partir de cette idée, Frank Rosenblatt a développé un algorithme d’apprentissage, qui consiste à entraîner un neurone artificiel sur des données de référence (X, y) pour que celui ci renforce ses paramètres w à chaque fois qu’une entrée X est activé en même temps que la sortie y présente dans ces données. Pour ça, il a imaginé la formule suivante, dans laquelle les paramètres w sont mis à jour en calculant la différence entre la sortie de référence et la sortie produite par le neurone, et en multipliant cette différence par la valeur de chaque entrée X, ainsi que par un pas d’apprentissage positif. De cette manière, si notre neurone produit une sortie différente de celle qu’il est censé produire, par exemple s’il nous sort y=0, alors qu’on voudrait avoir y=1, alors notre formule nous donnera w= w + alpha X. Donc, pour les entrées x qui valent 1, le coefficient w ce verra augmenté d’un petit pas alpha, il sera “renforcé” (pour reprendre les termes de la théorie de Hebb) ce qui provoquera une augmentation de la fonction w1 x1 + w2 x2… et qui rapprochera donc notre neurone de son seuil d’activation. Aussi longtemps que l’on sera en dessous de ce seuil, c’est à dire aussi longtemps que le neurone produira une mauvaise sortie, alors le coefficient w continuera d’augmenter grâce à notre formule, jusqu’au moment où y_true vaudra y… et à ce moment là notre formule donnera w = w + 0 ! Ce qui fait que nos paramètres arrêteront d’évoluer. Et voilà ! C’est ainsi que Frank Rosenblatt a développé le premier algorithme d’apprentissage de l’histoire du Deep Learning."
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#classification-à-laide-dun-perceptron",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#classification-à-laide-dun-perceptron",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Classification à l’aide d’un Perceptron",
    "text": "Classification à l’aide d’un Perceptron\nLa classification binaire\n\n\nSource : Machine Learnia\n\n\nPrenons un exemple : imaginer que l’on ait deux types de plantes, des plantes toxiques que l’on note y égal 1 et d’autres non toxique que l’on note y égal à zéro. On décide de mesurer certains attributs de ces plantes telles que la longueur et la largeur de leurs feuilles que l’on note x1 et x2. En représentant les résultats dans un graphique on observe que les deux classes de plantes sont linéairement séparables, on peut donc développé un modèle capable de prédire à quelle classe appartient une future plantes en se basant sur cette droite qu’on appelle la frontière de décision : si une plante se trouve à gauche elle sera considérée comme toxique appartenant à la classe y égal 1 et sinon elle sera considérée comme non toxique appartenant à la classe y égal zéro. Pour cela on va utiliser un modèle linéaire en fournissant comme tout à l’heure aux variables x1 et x2 à un neurone et en multipliant au passage chaque entrée du neurone par un poids w dans ce neurones et un coefficient complémentaires qu’on appelle le biais; ce qui nous donne une fonction z de x1, x2 égal w 1 x + w 2 x 2 + b. Sur notre graphique on peut colorer les régions où cette fonction retourne une valeur positive et celle où elle nous retourne une valeur négative. On constate alors que la frontière de décision correspond aux valeurs de x1 et x2 pour lesquels z est égal à zéro. Du coup pour prédire à quelle classe appartient une future plantes il va falloir régler les paramètres w et b de façon à séparer du mieux possible nos deux classes, après quoi on pourra dire si une plante est dans la classe 0 ou 1 en regardant simplement le signe de z, si négatif alors la plante sera dans la classe zéro et si positif elle sera dans la classe 1."
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#classification-à-laide-dun-perceptron-1",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#classification-à-laide-dun-perceptron-1",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Classification à l’aide d’un Perceptron",
    "text": "Classification à l’aide d’un Perceptron\nLa fonction d’activation (sigmoïde) et la fonction de coût (vraisemblance)\n\n\nSource : Machine Learnia\n\n\nPour améliorer ce modèle une bonne chose à faire serait d’accompagner chaque prédiction d’une probabilité. Plus une plante sera éloignée de la frontière de décision plus il sera évident c’est à dire probable qu’elles appartiennent bien à sa classe. Pour ça on pourrait utiliser une fonction d’activation nous retournant une sortie qui s’approchent de zéro ou un au fur et à mesure que l’on s’éloigne de la frontière de décision là où z est égal à zéro. Cette fonction qui nous permet de faire ça c’est la fonction sigmoïde également appelé fonction logistique, dont l’expression est égale … Cette fonction permet de convertir la sortie z en une probabilité qu une plante appartiennent à la classe 1, par exemple si nous avons une plante dont la valeur z est égale à 1,4 alors cela donne une probabilité égale à 0,8 ce qui signifie que d’après notre modèle cette plante a 80% de chance d’appartenir à la classe 1. C’est une probabilité relativement élevée, ce qui est logique vu que cette plante se situe à droite de la frontière de décision, là où nous sommes censés obtenir des plantes toxiques. A l’inverse si nous avons une plante dont la valeur de z est égal à moins 2,1 alors cela donne une probabilité égale à 0,1 ce qui signifie que d’après notre modèle cette plante a seulement 10% de chance d’appartenir à la classe 1, c’est une probabilité bien plus faible que tout à l’heure, mais encore une fois ça reste tout à fait logique vu que cette plante se situe à gauche de la frontière de décision là où nous ne sommes pas censés obtenir de plantes toxiques mais uniquement des plantes appartenant à la classe zéro. Du coup on pourra dire à la place que cette plante aura 90 % de chances d’être non toxiques soit la probabilité complémentaires à celles que nous avons calculée.\nCes probabilités suivent en fait une loi de bernoulli c’est à dire que la probabilité qu’une plante appartienne à la classe a est donnée par la probabilité qu’une plante appartienne à la classe zéro est donnée par 1 moins a(z). Le tout peut être résumé en une seule formule : il suffit de décomposer les deux cas, celui où y est égal à 1 et celui où y est égal à zéro et alors on voit qu’on retombe tout simplement sur les deux expressions de tout à l’heure.\nDonc pour résumer tout ce qu’on vient de voir : ce qu’on trouve à l’intérieur des neurones c’est une fonction linéaire z égal w 1 x 1 + w 2 x 2 + b suivie d’une fonction d’activation, la plus simple étant la fonction sigmoïde qui nous retourne une probabilité suivant une loi de bernoulli. Maintenant notre but ça va être de régler les paramètres w et de façon à obtenir le meilleur modèle possible c’est à dire le modèle qui fait les plus petites erreurs entre les sorties a(z) et les vraies données y. Et pour ça on va commencer par définir une fonction coût qui va permettre de mesurer ces erreurs. En machine learning une fonction coût, ou loss en anglais, c’est une fonction qui permet de quantifier les erreurs effectuées par un modèle. Dans notre cas c’est donc une fonction qui permet de mesurer les distances que l’on voit ici en rouge entre les sorties a(z) et les données y dont nous disposons. Pour ça la fonction coût que l’on va utiliser c’est la fonction de log loss, que l’on obtient par un calcul de maximum de vraisemblance. Enfin on effectue une descente de gradient pour minimiser le coût et déterminer les paramètres optimaux."
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#le-perceptron-multicouche",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#le-perceptron-multicouche",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Le Perceptron Multicouche",
    "text": "Le Perceptron Multicouche\n… et la rétropropagation du gradient (Rumelhart, Hinton, and Williams 1986)\n\n\nSource : Machine Learnia\n\n\nLe perceptron est un modèle linéaire et c’est là sa limitation. On connut alors le premier hiver de l’intelligence artificielle, de 1974 à 1980, période durant laquelle il n’y eu quasiment plus d’investisseurs pour financer les recherches en I.A. L’intelligence artificielle était sur le point de mourir… mais tout changea dans les années 80 lorsque Geoffrey Hinton, un des pères du Deep Learning, développa le Perceptron multicouches, le premier véritable réseau de neurones artificiels ! Le Perceptron Multicouches de Geoffrey Hinton. Comme je vous l’ai dit à l’instant, le Perceptron est en fait un modèle linéaire. Le seul ennui, c’est qu’une grande partie des phénomènes de notre univers ne sont pas des phénomènes linéaires. Et dans ces conditions, le Perceptron à lui seul n’est pas très utile. Mais rappelez-vous l’idée de McCulloch et Pitts : en connectant ensemble plusieurs neurones, il est possible de résoudre des problèmes plus complexes qu’avec un seul. Voyons donc ce qu’il se passe si l’on connecte par exemple 3 Perceptrons ensemble. Les 2 premiers reçoivent chacun les entrées x1 et x2. Ils font leur petit calcul, en fonction de leurs paramètres, et retournent une sortie y qu’ils envoient à leur tour vers le troisième Perceptron, qui va lui aussi faire ses petits calculs pour produire une sortie finale. Eh bien si l’on trace la représentation graphique de la sortie finale en fonction des entrées x1 x2, on obtient cette fois ci un modèle non linéaire qui est bien plus intéressant. Avec cet exemple, vous avez là votre premier réseau de neurones artificiels : 3 neurones, répartis en 2 couches (une couche d’entrée et une couche de sortie) c’est ce qu’on appelle un Perceptron Multicouche? Et des couches et des neurones, vous pouvez en rajouter autant que vous voulez ! Plus vous en remettrez, plus le résultat à la sortie sera complexe et intéressant. Cependant, une question subsiste… Comment entraîner un tel réseau de neurones pour qu’il fasse ce qu’on lui demande de faire ? C’est à dire, comment trouver les valeurs de tous les paramètres w et b de façon à obtenir un bon modèle ? Eh bien, la solution est d’utiliser une technique appelée Back Propagation, qui consiste à déterminer comment la sortie du réseau varie en fonction des paramètres présents dans chaque couche du modèle. Pour ça, on calcule une chaîne de gradients, indiquant comment la sortie varie en fonction de la dernière couche, puis comment la dernière couche varie en fonction de l’avant dernière, puis comment l’avant dernière varie en fonction de l’avant avant dernière etc… … jusqu’à arriver à la toute première couche de notre réseau. C’est une Back Propagation : une propagation vers l’arrière ! Avec ces informations, ces gradients, on peut alors mettre à jour les paramètres de chaque couche, de telle sorte à ce qu’ils minimisent l’erreur entre la sortie du modèle est la réponse attendue (la fameuse valeur y_true) par une descente de Gradient, dont nous parlerons plus en détail dans les prochaines vidéos. En résumé, pour développer et entraîner des réseaux de neurones artificiels, on répète en boucle les quatre étapes suivantes : La première étape, c’est l’étape de Forward Propagation : on fait circuler les données de la première couche jusqu’à la dernière, afin de produire une sortie y. La deuxième étape, c’est de calculer l’erreur entre cette sortie et la sortie de référence y_true que l’on désire avoir. Pour ça on utilise une fonction Coût. Ensuite, la troisième étape, c’est celle de la Back Propagation : on mesure comment cette fonction coût varie par rapport à chaque couche de notre modèle, en partant de la dernière et en remontant jusqu’à la toute première. Pour finir, la quatrième et dernière étape, c’est de corriger chaque paramètre du modèle grâce à l’algorithme de la descente de gradient, avant de re-boucler vers la première étape, celle de la Forward Propagation, pour recommencer un cycle d’entraînement."
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#lefficacité-du-perceptron-multicouche",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#lefficacité-du-perceptron-multicouche",
    "title": "Introduction à l’intelligence artificielle",
    "section": "L’efficacité du Perceptron Multicouche",
    "text": "L’efficacité du Perceptron Multicouche\n3 perceptrons pour créer une forme triangulaire\n\n\nSource : (Elgendy 2020)"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#lefficacité-du-perceptron-multicouche-1",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#lefficacité-du-perceptron-multicouche-1",
    "title": "Introduction à l’intelligence artificielle",
    "section": "L’efficacité du Perceptron Multicouche",
    "text": "L’efficacité du Perceptron Multicouche\nLinéariser les frontières de décision complexes\n\n\nSource : Towards Data Science"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#représentation-dun-perceptron-multicouche",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#représentation-dun-perceptron-multicouche",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Représentation d’un Perceptron Multicouche",
    "text": "Représentation d’un Perceptron Multicouche\nUn simple réseau de neurones à 3 couches\n\n\nSource : (Elgendy 2020)"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#formalisation-dun-perceptron-multicouche",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#formalisation-dun-perceptron-multicouche",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Formalisation d’un Perceptron Multicouche",
    "text": "Formalisation d’un Perceptron Multicouche\nCascades de multiplication matricielles et d’activation\n\n\nSource : (Elgendy 2020)"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#formalisation-dun-perceptron-multicouche-1",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#formalisation-dun-perceptron-multicouche-1",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Formalisation d’un Perceptron Multicouche",
    "text": "Formalisation d’un Perceptron Multicouche\nCascades de multiplication matricielles et d’activation\n\n\nSource : 3blue1brown"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#que-se-passe-t-il-dans-un-réseau-de-neurones",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#que-se-passe-t-il-dans-un-réseau-de-neurones",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Que se passe-t-il dans un réseau de neurones ?",
    "text": "Que se passe-t-il dans un réseau de neurones ?\nVisualisation de la linéarisation de la frontière de décision\n\n\nSource : Alexandre TL\n\n\nPour visualiser l’effet d’une couche on va se placer en dimension 2, on va donc seulement pouvoir visualiser des réseaux de neurones dont chaque couche possède deux neurones, mais on va voir que ça reste quand même assez intéressant. Prenons ainsi ce réseau de neurones composé de trois couches, qui était entraîné à réaliser la tâche de classification binaire sur ces données, donc c’est à la fois les données d’entraînement constituées des points de coordonnées x1x2 et leur classe associée représentée par une couleur, mais en plus des données on voit les prédictions du modèle entraîné avec, donc la couleur du fond, et donc on voit que le modèle arrive bien à prédire ces données. Évidemment ce problème de classification n’est pas linéaire c’est à dire qu’on ne peut pas séparer les deux classes par une simple droite, c’est pour ça qu’on a utilisé en réseau neurones; et ce qui va être intéressant c’est de voir comment ce réseau se débrouille pour fournir à la couche de sortie des données qui sont linéairement séparables, puisqu’on le sait la couche de sortie elle est équivalente à un seul modèle de régression logistique qui on l’a vu est un modèle linéaire. Alors commençons par visualiser les données en elle-même : on représente chaque exemple de données d’entraînement par un point, mais il faut bien voir cela comme un vecteur, donc là ce qu’on voit à l’écran c’est tout un ensemble de vecteurs et on va voir au travers de différentes visualisations comment le réseau modifie ces vecteurs. Évidemment le réseau n’a pas accès au classe c’est à dire aux couleurs de ces vecteurs mais seulement aux vecteurs en eux-mêmes. Ces vecteurs on va leur faire passer la première étape de la première couche c’est à dire la multiplication à gauche par la matrice de poids W. On voit que la matrice double V1 effectuant un agrandissement ainsi qu’une réflexion sur tous les vecteurs, ensuite on ajoute B1 le biais de la couche 1 donc là il s’agit seulement de déplacer l’ensemble des vecteurs dans la même direction puisqu’on ajoute coordonnées par coordonnées les poids de B et enfin on applique la fonction d’activation ici la fonction tangente hyperbolique qui ajoute de la non linarité dans le réseau. On le voit les vecteurs proches de l’origine sont très peu affectés mais les vecteurs avec une plus grande composante sont eux rapprochées de l’origine. La couche une est donc terminée et on voit que déjà les données sont linéairement séparables donc en fait on n’aurait pas besoin de deuxième couche cachée puisque le travail est déjà terminé mais on va quand même continuer avec une deuxième couche cachée. On multiplie donc par la matrice W2 on ajoute le biais et on applique la fonction tangente hyperbolique. On arrive alors sur la dernière couche, cette fois on passe en une dimension sur la droite puisque la sortie est un nombre. On passe en dimension une grâce à la matrice W3 puis on ajoute B3 et enfin on applique la fonction sigmoïde et là on voit clairement le travail effectué par le NN : tous les points de la classe bleue sont très proches de 1 et les points rouges sont très proches de 0 donc je le rappelle la sortie du modèle correspond à la probabilité d’appartenance à la classe une donc là on voit que les sorties du modèle sont pertinentes puisqu’il arrive à classifier correctement les données d’entraînement."
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#lefficacité-du-perceptron-multicouche-2",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#lefficacité-du-perceptron-multicouche-2",
    "title": "Introduction à l’intelligence artificielle",
    "section": "L’efficacité du Perceptron Multicouche",
    "text": "L’efficacité du Perceptron Multicouche\nApprendre des frontières de décision complexes\n\n\nSource : Playground Tensorflow"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#pourquoi-les-réseaux-de-neurones-fonctionnent",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#pourquoi-les-réseaux-de-neurones-fonctionnent",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Pourquoi les réseaux de neurones fonctionnent ?",
    "text": "Pourquoi les réseaux de neurones fonctionnent ?\nParce qu’ils approchent des fonctions complexes\n\n\nSource : DataMListic"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#pourquoi-les-réseaux-de-neurones-fonctionnent-1",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#pourquoi-les-réseaux-de-neurones-fonctionnent-1",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Pourquoi les réseaux de neurones fonctionnent ?",
    "text": "Pourquoi les réseaux de neurones fonctionnent ?\nParce qu’ils approchent des fonctions complexes\n\n\nSource : Emergent Garden"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#théorème-dapproximation-universelle",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#théorème-dapproximation-universelle",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Théorème d’approximation universelle",
    "text": "Théorème d’approximation universelle\nFonctions continues approchables par un réseau de neurones à 2 couches\n\n\n\n\n\n\nThéorème (Cybenko, 1989, Hornik, 1991)\n\n\nSoit \\(\\sigma:\\mathbb{R}\\rightarrow \\mathbb{R}\\) une fonction non constante, bornée et continue. Soit \\(I_m\\) le cube unité \\(m\\)-dimensionnel \\([0, 1]^m\\). L’espace des fonctions continues à valeurs réelles sur \\(I_m\\) est noté \\(C(I_m)\\). Alors, pour tout \\(\\epsilon &gt; 0\\) et toute fonction \\(f \\in C(I_m)\\), il existe un entier \\(N\\), des constantes réelles \\(v_i, b_i \\in \\mathbb{R}\\) et des vecteurs réels \\(\\boldsymbol{w}_i \\in \\mathbb{R}^m\\) pour \\(i = 1, \\dotsc, N\\), tels que nous puissions définir :\n\\[ F(\\boldsymbol{x}) = \\sum_{i=1}^N v_i \\sigma\\left(\\boldsymbol{w}_i^T \\boldsymbol{x}+b_i\\right)=\\boldsymbol{v}^T \\sigma\\left(\\mathbf{W}^T \\boldsymbol{x}+\\boldsymbol{b}\\right)\\] comme une approximation de la fonction \\(f\\), c’est-à-dire, \\[ |f(\\boldsymbol{x})-F(\\boldsymbol{x})|&lt;\\epsilon, \\quad \\forall \\boldsymbol{x}\\in I_m\\]"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#résumé-de-lapprentissage-supervisé",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#résumé-de-lapprentissage-supervisé",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Résumé de l’apprentissage supervisé",
    "text": "Résumé de l’apprentissage supervisé\nPoint de vue général\n\nDes données d’entrainement \\((\\boldsymbol{x}_1,y_1),\\dotsc,(\\boldsymbol{x}_n,y_n)\\)\nUn modèle, une famille de fonctions \\(f\\in \\mathcal{H}\\) assurant \\(y_i\\approx f(\\boldsymbol{x}_i)\\)\nUne fonction de coût \\(\\ell\\) mesurant la qualité de l’approximation\nUn algorithme d’optimisation trouvant \\(f\\in \\mathcal{H}\\) qui minimise le coût \\[ \\min_{f\\in \\mathcal{H}} \\frac{1}{n} \\sum_{i=1}^n \\ell(y_i, f(\\boldsymbol{x}_i))\\]"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#résumé-de-lapprentissage-supervisé-1",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#résumé-de-lapprentissage-supervisé-1",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Résumé de l’apprentissage supervisé",
    "text": "Résumé de l’apprentissage supervisé\nPoint de vue des réseaux de neurones\n\nDes données d’entrainement \\((\\boldsymbol{x}_1,y_1),\\dotsc,(\\boldsymbol{x}_n,y_n)\\)\nUn réseau à \\(k\\) neurones soit un groupe de poids \\((\\boldsymbol{w}_1,\\dotsc \\boldsymbol{w}_k, b_1, \\dotsc, b_k)\\) assurant \\[y_i\\approx \\{NN(\\boldsymbol{w}_1,\\dotsc \\boldsymbol{w}_k, b_1, \\dotsc, b_k)\\}(\\boldsymbol{x}_i)\\]\nUne fonction de coût \\(\\ell\\) mesurant la qualité de l’approximation\nUn algorithme d’optimisation trouvant les poids \\((\\boldsymbol{w}_1,\\dotsc \\boldsymbol{w}_k, b_1, \\dotsc, b_k)\\) qui minimisent le coût \\[ \\min_{(\\boldsymbol{w}_k, b_k)} \\frac{1}{n} \\sum_{i=1}^n \\ell(y_i, \\{NN(\\boldsymbol{w}_1,\\dotsc \\boldsymbol{w}_k, b_1, \\dotsc, b_k)\\}(\\boldsymbol{x}_i))\\]"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#classification-dimages",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#classification-dimages",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Classification d’images",
    "text": "Classification d’images\nReconnaissance de chiffres (MNIST)\n\n\nSource : (Elgendy 2020)"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#image-en-couleurs",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#image-en-couleurs",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Image en couleurs",
    "text": "Image en couleurs\n3 canaux RGB\n\n\nSource : (Elgendy 2020)"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#classification-dimages-1",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#classification-dimages-1",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Classification d’images",
    "text": "Classification d’images\nExtraction de caractéristiques\n\n\nSource : (Elgendy 2020)"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#classification-dimages-2",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#classification-dimages-2",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Classification d’images",
    "text": "Classification d’images\nMéthode traditionnelle (handcrafted features) vs moderne (deep learning)\n\n\n\n\n\n\n\n\n\n\n\nSource : (Elgendy 2020)"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#classification-dimages-3",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#classification-dimages-3",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Classification d’images",
    "text": "Classification d’images\nDiscriminer deux chiffres\n\n\nSource : (Elgendy 2020)"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#briques-de-base-dun-cnn",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#briques-de-base-dun-cnn",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Briques de base d’un CNN",
    "text": "Briques de base d’un CNN\nLes filtres de convolution\n\n\nSource : (Elgendy 2020)"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#briques-de-base-dun-cnn-1",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#briques-de-base-dun-cnn-1",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Briques de base d’un CNN",
    "text": "Briques de base d’un CNN\nLes filtres de convolution\n\n\nSource : (Elgendy 2020)"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#briques-de-base-dun-cnn-2",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#briques-de-base-dun-cnn-2",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Briques de base d’un CNN",
    "text": "Briques de base d’un CNN\nLes filtres de convolution\n\n\nSource : (Elgendy 2020)"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#briques-de-base-dun-cnn-3",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#briques-de-base-dun-cnn-3",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Briques de base d’un CNN",
    "text": "Briques de base d’un CNN\nLes filtres de convolution\n\n\nSource : (Elgendy 2020)"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#briques-de-base-dun-cnn-4",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#briques-de-base-dun-cnn-4",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Briques de base d’un CNN",
    "text": "Briques de base d’un CNN\nLes filtres de convolution\n\n\nSource : (Elgendy 2020)"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#briques-de-base-dun-cnn-5",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#briques-de-base-dun-cnn-5",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Briques de base d’un CNN",
    "text": "Briques de base d’un CNN\nLe max pooling\n\n\nSource : CS231n"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#briques-de-base-dun-cnn-6",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#briques-de-base-dun-cnn-6",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Briques de base d’un CNN",
    "text": "Briques de base d’un CNN\nLe max pooling\n\n\nSource : CS231n"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#classification-dimages-4",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#classification-dimages-4",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Classification d’images",
    "text": "Classification d’images\nDiscriminer les chiffres MNIST (LeCun et al. 1989b, 1989a, 1998)\n\n\nSource : (Elgendy 2020)"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#classification-dimages-5",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#classification-dimages-5",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Classification d’images",
    "text": "Classification d’images\nDiscriminer les chiffres MNIST\n\n\nSource : 3blue1brown"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#deep-learning",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#deep-learning",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Deep learning",
    "text": "Deep learning\nAlexNet (Krizhevsky, Sutskever, and Hinton 2012)\n\n\nSource : (Elgendy 2020)"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#deep-learning-1",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#deep-learning-1",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Deep learning",
    "text": "Deep learning\nLa première couche du réseau\n\n\nSource : (Krizhevsky, Sutskever, and Hinton 2012)"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#deep-learning-2",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#deep-learning-2",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Deep learning",
    "text": "Deep learning\nLes couches du réseau extraient des features à différentes échelles\n\n\nSource : Medium"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#deep-learning-3",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#deep-learning-3",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Deep learning",
    "text": "Deep learning\nLes couches du réseau extraient des features à différentes échelles\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSource : (Elgendy 2020)"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#deep-learning-4",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#deep-learning-4",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Deep learning",
    "text": "Deep learning\nLes couches du réseau extraient des features à différentes échelles\n\n\nSource : 3blue1brown"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#deep-learning-5",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#deep-learning-5",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Deep learning",
    "text": "Deep learning\nLes CNN en résumé\n\n\nSource : (Maried, Omar, and Baba 2017)"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#deep-learning-6",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#deep-learning-6",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Deep learning",
    "text": "Deep learning\nAlexNet breakthrough\n\n\nSource : Welch Labs\n\n\nDans les blocs convolutionnels, le tenseur d’image en entrée est transformé en faisant glisser un tenseur beaucoup plus petit, appelé noyau, constitué de valeurs de poids apprises, sur l’image. À chaque position, le produit scalaire est calculé entre l’image et le noyau. Ici, il est utile de considérer le produit scalaire comme une mesure de similarité : plus un patch de l’image et le noyau sont similaires, plus le produit scalaire sera élevé.\nAlexNet utilise 96 noyaux individuels dans sa première couche, chacun ayant une dimension de 11 par 11 par 3. On peut donc les visualiser facilement sous forme de petites images RGB. Ces images donnent une bonne idée de la façon dont la première couche d’AlexNet perçoit l’image. Les noyaux situés en haut de cette figure montrent qu’AlexNet a appris à détecter des contours ou des transitions rapides entre clair et sombre à différents angles. Les images avec des motifs similaires produiront des produits scalaires élevés avec ces noyaux. En bas, on observe qu’AlexNet a appris à détecter des zones de différentes couleurs. Ces noyaux sont initialisés aléatoirement et les motifs que nous voyons sont entièrement appris à partir des données.\nEn faisant glisser chacun de nos 96 noyaux sur l’image d’entrée et en calculant le produit scalaire à chaque position, on obtient un nouvel ensemble de 96 matrices, parfois appelées cartes d’activation. Heureusement, on peut également visualiser ces cartes comme des images. Les cartes d’activation montrent quelles parties d’une image, si elles existent, correspondent bien à un noyau donné. Si je montre un motif visuellement similaire à un noyau donné, on observe une activation élevée dans cette partie de la carte d’activation. Cette activation disparaît si je fais pivoter le motif de 90°, car l’image et le noyau ne sont plus alignés.\nOn peut également voir différentes cartes d’activation détecter des contours et d’autres caractéristiques simples dans notre image. Bien entendu, détecter des contours et des zones de couleur dans les images est encore très éloigné de la reconnaissance de concepts complexes comme des bergers allemands ou des porte-avions. Ce qui est remarquable avec les réseaux neuronaux profonds comme AlexNet (ou ChatGPT), c’est que l’on répète simplement cette opération encore et encore, mais avec un ensemble différent de poids appris.\nCela signifie que ces 96 cartes d’activation sont empilées ensemble pour former un tenseur qui devient l’entrée d’un bloc de calcul convolutionnel du même type, dans la deuxième couche du modèle. On peut rendre les activations plus faciles à visualiser en supprimant les valeurs proches de zéro. Malheureusement, dans la deuxième couche, il est difficile d’apprendre quelque chose simplement en visualisant les valeurs des poids et les noyaux eux-mêmes.\nLe premier problème est que nous ne pouvons pas voir suffisamment de couleurs. La profondeur du noyau doit correspondre à la profondeur des données entrantes. Dans la première couche d’AlexNet, la profondeur des données entrantes est de trois, car le modèle prend en entrée des images en couleur avec des canaux rouge, vert et bleu. Cependant, comme la première couche calcule 96 cartes d’activation distinctes, le calcul dans la deuxième couche d’AlexNet revient à traiter des images avec 96 canaux de couleur distincts.\nLe deuxième facteur qui rend la compréhension de ce qui se passe dans la deuxième couche plus difficile est que les produits scalaires effectuent en réalité des combinaisons pondérées des calculs réalisés dans la première couche. Nous avons besoin d’un moyen de visualiser comment les couches interagissent. Une méthode simple pour voir ce qui se passe est d’identifier les parties de diverses images qui activent fortement les sorties de la deuxième couche. Par exemple, cette carte d’activation semble assembler des détecteurs de contours pour former des coins basiques.\nDe manière remarquable, à mesure que nous progressons dans AlexNet, les activations fortes correspondent à des concepts de plus en plus abstraits. Lorsque nous atteignons la cinquième couche, nous obtenons des cartes d’activation qui répondent très fortement aux visages et à d’autres concepts de haut niveau. Ce qui est incroyable ici, c’est qu’AlexNet n’a jamais été explicitement programmé pour reconnaître un visage. Tout ce qu’AlexNet a appris provient des images et des étiquettes du jeu de données ImageNet, qui ne contient pas de classe spécifique pour une personne ou un visage. AlexNet a été capable d’apprendre, de manière totalement autonome, à la fois que les visages sont importants et comment les reconnaître.\nPour mieux comprendre ce qu’un noyau donné dans AlexNet a appris, nous pouvons également examiner les exemples dans le jeu de données d’entraînement qui génèrent les valeurs d’activation les plus élevées pour ce noyau. Pour un noyau dédié aux visages, il n’est pas surprenant que nous trouvions des exemples contenant des personnes."
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#deep-learning-7",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#deep-learning-7",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Deep learning",
    "text": "Deep learning\nUn déluge de données\n\n1,2 millions d’images d’entrainement\n100 000 images de test\n\n\n\nSource : Andrej Karpathy"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#deep-learning-8",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#deep-learning-8",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Deep learning",
    "text": "Deep learning\n1000 classes d’objets dans ImageNet\n\n\nSource : (Krizhevsky, Sutskever, and Hinton 2012)\n\n\nCapcha"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#deep-learning-9",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#deep-learning-9",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Deep learning",
    "text": "Deep learning\nLe succès des CNN en classification d’images\n\n\nSource : (Allen et al. 2019)"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#deep-learning-10",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#deep-learning-10",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Deep learning",
    "text": "Deep learning\nL’une des raisons de ce succès : les ressouces computationnelles\n\n\nSource : Welch Labs\n\n\nLes algorithmes qui existent depuis les années 1980 fonctionnent très bien, mais cela n’était pas évident avant 2006. Le problème était peut-être simplement que ces algorithmes étaient trop coûteux en termes de calcul pour permettre beaucoup d’expérimentations avec le matériel disponible à l’époque. La grande différence en 2012 était simplement l’échelle des données et celle de la puissance de calcul. Le jeu de données ImageNet était le plus grand ensemble de données étiquetées de ce genre à ce jour, avec plus de 1,3 million d’images. Et grâce aux GPU de Nvidia, l’équipe de Hinton en 2012 avait accès à environ 10 000 fois plus de puissance de calcul que celle dont disposait Yann LeCun 15 ans auparavant.\nLe modèle LeNet-5 de LeCun comptait environ 60 000 paramètres apprenables. AlexNet a augmenté ce nombre d’un facteur mille, atteignant environ 60 millions de paramètres. Aujourd’hui, ChatGPT compte bien plus d’un trillion de paramètres, soit plus de 10 000 fois la taille d’AlexNet. Cette échelle vertigineuse est la marque de fabrique de cette troisième vague d’IA dans laquelle nous nous trouvons, à la fois moteur de leurs performances et de la difficulté fondamentale à comprendre comment ces modèles parviennent à leurs résultats.\nIl est étonnant que nous puissions comprendre qu’AlexNet apprend des représentations de visages et que les grands modèles de langage apprennent des représentations de concepts comme le Golden Gate Bridge. Mais ces modèles apprennent de nombreux autres concepts pour lesquels nous n’avons même pas de mots. Les atlas d’activation sont fascinants et magnifiques, mais restent des projections très basses dimensions d’espaces très haute dimension, où nos capacités de raisonnement spatial s’effondrent souvent.\nIl est notoirement difficile de prédire où l’IA ira ensuite. Presque personne n’avait prévu que les réseaux neuronaux des années 1980 et 1990, mis à l’échelle de trois ou quatre ordres de grandeur, donneraient naissance à AlexNet. Et il était presque impossible de prévoir qu’une généralisation des blocs de calcul d’AlexNet, mise à l’échelle de plusieurs ordres de grandeur, mènerait à ChatGPT. Peut-être que la prochaine avancée en IA est juste à trois ou quatre ordres de grandeur d’échelle supplémentaires, ou peut-être qu’une approche d’IA presque oubliée refera surface, comme AlexNet en 2012. Il faudra attendre et voir."
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#qui-suis-je",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#qui-suis-je",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Qui suis-je ?",
    "text": "Qui suis-je ?\nUn esprit visionnaire sur les potentialités des machines\n“Again, it might act upon other things besides number, were objects found whose mutual fundamental relations could be expressed by those of the abstract science of operations, and which should be also susceptible of adaptations to the action of the operating notation and mechanism of the engine. Supposing, for instance, that the fundamental relations of pitched sounds in the science of harmony and of musical composition were susceptible of such expression and adaptations, the engine might compose elaborate and scientific pieces of music of any degree of complexity or extent”\nA.L.L\n\nSource : Sketch of the analytical engine\n\n\nAu commencement était le verbe n’est-ce pas, donc je vais laisser la parole à cette personnalité énigmatique que je pose ici en devinette, et qui a eu ces paroles quasi prophétiques il y a plus de 150 ans au sujet de la potentialité des machines à manipuler des concepts :\n“Encore une fois, cela pourrait agir sur d’autres éléments en plus des nombres, si l’on trouvait des objets dont les relations fondamentales mutuelles pourraient être exprimées par celles de la science abstraite des opérations, et qui seraient également susceptibles d’adaptations à l’action de la notation et du mécanisme opérant de la machine. En supposant, par exemple, que les relations fondamentales entre les fréquences dans la science de l’harmonie et les règles de la composition musicale soient susceptibles d’une telle expression, la machine pourrait composer des morceaux de musique élaborés et scientifiques, de n’importe quel degré de complexité ou d’étendue.”\nPour le dire plus simplement, ce qu’a compris très précocément cet esprit visionnaire, c’est qu’à partir du moment où l’on pouvait automatiser une machine à réaliser des opérations abstraites, en premier lieu sur des nombres, alors si l’on est capable d’encoder à travers ces nombres des objets tels que de la musique, des images, des phrases, etc il devient alors envisageable de composer des morceaux de musiques, des images, des phrases, d’une grande complexité. Ce qui préfigure ce qu’on appelle et que l’on a découvert depuis seulement quelques années l’IA générative. Il n’en sera pas question dans cet exposé, mais je voulais souligner que cette potentialité était en germe dès l’avènement de la proto-informatique, et que cette vision prophétique on la doit à (vous l’avez reconnue ?) Lady Lovelace, très grand esprit de l’époque Victorienne qui a contribué à la machine analytique de Charles Babbage, et dont la postérité la rendue célèbre pour ses fameuses notes sur cette machine (et la trace du premier programme informatique). Alan Turing dans son célèbre article de 1950 reprendra à son compte l’objection de Lady Lovelace."
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#ada-lovelace",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#ada-lovelace",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Ada Lovelace",
    "text": "Ada Lovelace\nL’enchanteresse des nombres\n\n\n\n\n\n\n\n\n\n1815 - Naissance d’Ada Lovelace à Londres, fille du poète Lord Byron et d’Anne Isabella Milbanke.\n1833 - Rencontre avec Charles Babbage, mathématicien et inventeur de la machine analytique.\n1842-1843 - Publication des Notes, ajoutées à la traduction d’un article du mathématicien italien Luigi Menabrea.\n1852 - Décès à l’âge de 36 ans, des suites d’un cancer.\n\n\n\nSource : Wikipédia\n\n\nun précurseur de l’ordinateur moderne. Cette rencontre marquera le début de sa fascination pour les mathématiques et les machines. Elle y ajoute ses propres notes, qui occupent plus de place que l’article lui-même et dans lesquelles elle propose ce qui est souvent considéré comme le premier algorithme conçu pour une machine, visant à calculer les nombres de Bernoulli. sur la machine analytique de Babbage. - 1843 - Paternité de la Programmation : Ses notes incluent des concepts de boucles et de programmation conditionnelle. Ces idées font d’elle la première personne à conceptualiser un algorithme pour un ordinateur, lui conférant le titre de “première programmeuse”."
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#modélisation-des-neurones",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#modélisation-des-neurones",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Modélisation des neurones",
    "text": "Modélisation des neurones\nNeurones biologiques vs neurones artificiels\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\nSource : AquaPortail - CS231n\n\n\npar exemple j’entends souvent dire qu’un neurone c’est comme un système binaire soit ça envoie un signal signal électrique soit pas alors que pas du tout il faut pas confrontre cette fonction et cette explication une synapse c’est infiniment plus complexe que un One Two que soit oui soit non ça c’est une synapse simplifiée il faut pas confondre fonction et fonctionnement j’ai une petite fille je lui ai montrer trois photos de chats et c’était assez pour qu’elle puisse reconnaître les chats les chiens et peut-être les oiseaux chadptier a dû bosser sur 45 TB donc à peu près 200 milliards de mots pour pouvoir comprendre quelque chose on n pas du tout les mêmes moyens d’apprentissage\nc’est pas parce que desa cré des fonctions qui sont similaires sur le cerveau qu’on peut dire qu’il fonctionneent pareil\nle vrai danger c’est que parce que ça fonctionne comme le cerveau c’est qu’on commence à y faire confiance plus donc il y a pas vraiment une guerre d’intelligence je fais pas des guerres contre ma calculette on fait pas des compétitions"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#modélisation-des-réseaux-de-neurones",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#modélisation-des-réseaux-de-neurones",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Modélisation des réseaux de neurones",
    "text": "Modélisation des réseaux de neurones\nLes CNN inspirés du fonctionnement du cortex visuel (Hubel and Wiesel 1962)\n\n\nSource : (Yamins and DiCarlo 2016)\n\n\n« En lisant un dialogue entre Noam Chomsky et Jean Piaget sur l’apprentissage inné ou acquis du langage, j’ai repéré un argument faisant référence aux réseaux de neurones que je ne connaissais pas. Ce champ prometteur était quelque peu abandonné et je m’y suis plongé tout seul »"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#modélisation-bio-inspirée",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#modélisation-bio-inspirée",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Modélisation bio-inspirée",
    "text": "Modélisation bio-inspirée\nL’oiseau vs l’avion\n“L’analogie peut être faite avec les pionniers de l’aviation, dont certains essayaient de reproduire les oiseaux ou les chauve-souris. Mais ils collaient un peu trop près à la biologie, comme Clément Ader. Ses travaux n’ont pas eu beaucoup de suites parce qu’il copiait les chauve-souris sans s’occuper de problèmes comme la stabilité. Alors que d’autres personnes plus proches des techniques d’ingénierie ont fait des expérimentations en soufflerie, ont essayé plusieurs profils d’ailes, etc. Et à la fin, ils ont obtenu un artefact, un avion, qui utilise les mêmes principes que les oiseaux pour voler mais dont les détails sont très différents. C’est un peu ce qu’on fait avec l’intelligence artificielle, on prend de l’inspiration avec ce qu’on observe dans le monde animal, mais on en dégage surtout des principes. On fabrique une machine dont le fonctionnement est finalement très différent de la biologie.”\nYann Lecun\n\nSource : Telescopemag"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#lia-pour-quoi-faire",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#lia-pour-quoi-faire",
    "title": "Introduction à l’intelligence artificielle",
    "section": "L’IA, pour quoi faire ?",
    "text": "L’IA, pour quoi faire ?\nExemples d’applications vertueuses\nEnvironnementales\n\nSmart grids\nOptimisation des transports, éclairage, chauffage, tri des déchêts, …\nGestion de la production agricole, images aériennes, …\nPrévoir les pics de pollution de l’air, feux de forêt, séismes, …\nConception de matériaux / habitats plus performants\nOptimisation du couvert végétal des villes\nModélisation du changement climatique, des écosystèmes, etc\n\n\nSource : cese"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#lia-pour-quoi-faire-1",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#lia-pour-quoi-faire-1",
    "title": "Introduction à l’intelligence artificielle",
    "section": "L’IA, pour quoi faire ?",
    "text": "L’IA, pour quoi faire ?\nExemples d’applications vertueuses\nSociales et sanitaires\n\nAccompagnement des personnes en situation de handicap\nAmélioration de la productivité\nDétection de maladies, chirurgie assistée par ordinateur\nMédecine préventive et personnalisée\nFouille de données génétiques\nPrédiction de la forme des protéines\nProduction de médicaments, vaccins, pharmacovigilance, …\n\n\nSource : cese\n\n\nforce est de constater que l’IA est surtout mise au service de domaines tels que la finance, le marketing ciblé et désormais l’industrie, et non pour entraîner et faire fonctionner des algorithmes d’optimisation bénéfiques à l’environnement. Et l’histoire d’Internet suggère que la plus grande partie des ressources utilisées par les SIA le seront pour générer des contenus de faible utilité sociale, voire pouvant renforcer des rapports sociaux inégalitaires (pour exemple : la pornographie)"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#les-problèmes-que-posent-lia",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#les-problèmes-que-posent-lia",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Les problèmes que posent l’IA",
    "text": "Les problèmes que posent l’IA\nUn pharmakon\n\nProduction massive de fausses informations, algorithmes de recommandations publicitaires, deep fakes, …\nReproduction de biais, application dans la justice\nRisques démocratiques : ciblages personnalisés lors des élections, hameçonnage, …\nDépendance technologique\n\nChômage structurel (?)\nCyberattaques\nSurveillance de masse\nDrônes et autres armes autonomes\n\n\nSource : cese"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#les-problèmes-que-posent-lia-1",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#les-problèmes-que-posent-lia-1",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Les problèmes que posent l’IA",
    "text": "Les problèmes que posent l’IA\nUne consommation considérable de ressources\n\nPhase d’entraînement (GPT-3) estimée à 552 tonnes CO2eq sur quinze jours (environ 200 allers-retours entre Paris et New York)\nPhase d’utilisation encore plus énergivore en volume (180 millions d’utilisateur de ChatGPT), IoT (100 milliards d’objets connectés)\nGoogle (14 MT CO2, +48% en 2023) : «À mesure que nous intégrons l’IA dans nos produits, la réduction des émissions pourrait s’avérer difficile»\nUtilisation de métaux rares, production de puces, GPU, data centers, …\nConsommation d’eau importante (le projet de data center de Meta à Talavera de la Reina, en Espagne, devrait prélever 665 millions de litres d’eau par an dans une région en plein stress hydrique).\nEffets rebonds (!)\n\n\nSource : cese\n\n\n2 500 000 km parcourus en voiture 150 000 tonnes d’eau par jour pour laver le silicium"
  },
  {
    "objectID": "slides/2024-11-21-intro-ia/intro-ia.html#bibliographie",
    "href": "slides/2024-11-21-intro-ia/intro-ia.html#bibliographie",
    "title": "Introduction à l’intelligence artificielle",
    "section": "Bibliographie",
    "text": "Bibliographie\n\n\n\n\n\n\n\n\nAllen, Bibb, Steven Seltzer, Curtis Langlotz, Keith Dreyer, Ronald Summers, Nicholas Petrick, Danica Marinac-Dabic, et al. 2019. “A Road Map for Translational Research on Artificial Intelligence in Medical Imaging: From the 2018 National Institutes of Health/RSNA/ACR/the Academy Workshop.” Journal of the American College of Radiology 16 (May). https://doi.org/10.1016/j.jacr.2019.04.014.\n\n\nBishop, Christopher M. 2006. “Pattern Recognition and Machine Learning.” Springer Google Schola 2: 1122–28.\n\n\nElgendy, Mohamed. 2020. Deep Learning for Vision Systems. Simon; Schuster.\n\n\nHubel, David H, and Torsten N Wiesel. 1962. “Receptive Fields, Binocular Interaction and Functional Architecture in the Cat’s Visual Cortex.” The Journal of Physiology 160 (1): 106.\n\n\nKrizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. “Imagenet Classification with Deep Convolutional Neural Networks.” Advances in Neural Information Processing Systems 25.\n\n\nLeCun, Yann, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. 1989a. “Backpropagation Applied to Handwritten Zip Code Recognition.” Neural Computation 1 (4): 541–51.\n\n\nLeCun, Yann, Bernhard Boser, John Denker, Donnie Henderson, Richard Howard, Wayne Hubbard, and Lawrence Jackel. 1989b. “Handwritten Digit Recognition with a Back-Propagation Network.” Advances in Neural Information Processing Systems 2.\n\n\nLeCun, Yann, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. “Gradient-Based Learning Applied to Document Recognition.” Proceedings of the IEEE 86 (11): 2278–2324.\n\n\nMaried, Esam, Osama Omar, and Abdullatif Baba. 2017. “A Literature Study of Deep Learning and Its Application in Digital Image Processing.” https://doi.org/10.13140/RG.2.2.17403.72480.\n\n\nMcCulloch, Warren S, and Walter Pitts. 1943. “A Logical Calculus of the Ideas Immanent in Nervous Activity.” The Bulletin of Mathematical Biophysics 5: 115–33.\n\n\nRosenblatt, Frank. 1958. “The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain.” Psychological Review 65 (6): 386.\n\n\nRumelhart, David E, Geoffrey E Hinton, and Ronald J Williams. 1986. “Learning Representations by Back-Propagating Errors.” Nature 323 (6088): 533–36.\n\n\nYamins, Daniel LK, and James J DiCarlo. 2016. “Using Goal-Driven Deep Learning Models to Understand Sensory Cortex.” Nature Neuroscience 19 (3): 356–65."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Migrer son blog Wordpress vers un blog Quarto statique hébergé gratuitement avec Github Pages",
    "section": "",
    "text": "Mon site perso fait peau neuve !\nJe vous explique ici quelles étapes j’ai suivi pour effectuer la migration. Au boulot !"
  },
  {
    "objectID": "posts/welcome/index.html#exporter-ses-articles-wordpress-en-html-vers-markdown",
    "href": "posts/welcome/index.html#exporter-ses-articles-wordpress-en-html-vers-markdown",
    "title": "Migrer son blog Wordpress vers un blog Quarto statique hébergé gratuitement avec Github Pages",
    "section": "Exporter ses articles Wordpress en HTML vers Markdown",
    "text": "Exporter ses articles Wordpress en HTML vers Markdown\nJ’ai expérimenté trois solutions :\n\nGitHub - SchumacherFM/wordpress-to-hugo-exporter\nGitHub - lonekorean/wordpress-export-to-markdown\nGitHub - palaniraja/blog2md\n\nLa première conserve certaines balises HTML (typiquement pour l’usage de la couleur et la mise en forme), tandis que les deux suivantes exportent en pur Markdown."
  },
  {
    "objectID": "posts/welcome/index.html#wordpress-to-hugo-exporter",
    "href": "posts/welcome/index.html#wordpress-to-hugo-exporter",
    "title": "Migrer son blog Wordpress vers un blog Quarto statique hébergé gratuitement avec Github Pages",
    "section": "Wordpress to Hugo Exporter",
    "text": "Wordpress to Hugo Exporter\nLa première solution consiste à uploader l’archive sur le site WP dans le dossier wp-content/plugins puis d’activer le plugin et utiliser Outils &gt; Export Hugo. Ce dernier n’a pas fonctionné donc j’ai utilisé le Terminal de mon serveur d’hébergement (O2switch) (comme expliqué ici) :\ncd wp-content/plugins/wordpress-to-hugo-exporter/\nphp hugo-export-cli.php\nLe script créé un fichier /tmp/wp-hugo.zip (cela peut prendre quelques minutes). Dans le gestionnaire de fichiers (via le Cpanel d’O2switch) j’ai effectué une recherche du fichier, il m’indique que celui-ci se trouve dans le dossier caché .cagefs/tmp/.\n\nWordpress export to Markdown\nPremièrement on effectue un export du contenu complet de WP au format export.xml, que l’on place dans l’archive du code téléchargé. Puis on exécute le script :\nnpm install && node index.js\n\n\nBlog2md\nDe même on exécute :\nnpm install && node index.js w export.xml out"
  },
  {
    "objectID": "posts/welcome/index.html#nettoyage-des-articles-exportés-liens-images",
    "href": "posts/welcome/index.html#nettoyage-des-articles-exportés-liens-images",
    "title": "Migrer son blog Wordpress vers un blog Quarto statique hébergé gratuitement avec Github Pages",
    "section": "Nettoyage des articles exportés, liens, images, …",
    "text": "Nettoyage des articles exportés, liens, images, …"
  },
  {
    "objectID": "posts/welcome/index.html#créer-son-blog-avec-quarto",
    "href": "posts/welcome/index.html#créer-son-blog-avec-quarto",
    "title": "Migrer son blog Wordpress vers un blog Quarto statique hébergé gratuitement avec Github Pages",
    "section": "Créer son blog avec Quarto",
    "text": "Créer son blog avec Quarto\n\nOuvrir RStudio\nCréer un nouveau projet Create Quarto Blog\nCocher Create a git repository\n\nUn blog Quarto vierge est ainsi créé contenant ces fichiers :\n\n_quarto.yml : Fichier du projet Quarto\nindex.qmd : Page d’accueil\nabout.qmd : Page “À propos”\nposts/ : Répertoire contenant les posts en Quarto Markdown (.qmd)\nposts/_metadata.yml : Options partagées des posts\nstyles.css : CSS customisé pour le style du blog\nMyBlog.Rproj : Raccourci d’ouverture du projet Quarto\n\nDans le fichier _quarto.yml ajoutez la ligne output-dir pour renseigner le répertoire cible pour la génération du site :\nproject:\n  type: website\n  output-dir: docs\nEnfin pour le générer et avoir le rendu du blog il suffit d’exécuter build &gt; Render Website"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Carnet de notes",
    "section": "",
    "text": "Migrer son blog Wordpress vers un blog Quarto statique hébergé gratuitement avec Github Pages\n\n\n\n\n\n\nsite\n\n\n\n\n\n\n\n\n\nDec 26, 2024\n\n\nKévin Polisano\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "slides/test.html#panorama-de-lintelligence-artificielle",
    "href": "slides/test.html#panorama-de-lintelligence-artificielle",
    "title": "Introduction à l’IA",
    "section": "Panorama de l’intelligence artificielle",
    "text": "Panorama de l’intelligence artificielle\n\n\nSource : test\n\n\nSpeaker notes go here."
  },
  {
    "objectID": "slides/test.html#qui-suis-je",
    "href": "slides/test.html#qui-suis-je",
    "title": "Introduction à l’IA",
    "section": "Qui suis-je ?",
    "text": "Qui suis-je ?\n“Again, it might act upon other things besides number, were objects found whose mutual fundamental relations could be expressed by those of the abstract science of operations, and which should be also susceptible of adaptations to the action of the operating notation and mechanism of the engine. Supposing, for instance, that the fundamental relations of pitched sounds in the science of harmony and of musical composition were susceptible of such expression and adaptations, the engine might compose elaborate and scientific pieces of music of any degree of complexity or extent”"
  },
  {
    "objectID": "slides/test.html#lenchanteresse-des-nombres",
    "href": "slides/test.html#lenchanteresse-des-nombres",
    "title": "Introduction à l’IA",
    "section": "L’enchanteresse des nombres",
    "text": "L’enchanteresse des nombres"
  },
  {
    "objectID": "slides/test.html#test",
    "href": "slides/test.html#test",
    "title": "Introduction à l’IA",
    "section": "test",
    "text": "test"
  },
  {
    "objectID": "slides/test.html#test-3",
    "href": "slides/test.html#test-3",
    "title": "Introduction à l’IA",
    "section": "test 3",
    "text": "test 3"
  },
  {
    "objectID": "slides/test.html#before-you-proceed",
    "href": "slides/test.html#before-you-proceed",
    "title": "Introduction à l’IA",
    "section": "Before you proceed…",
    "text": "Before you proceed…\nRequirements for the coding examples in this demo\nThe clean theme is language agnostic. Use it with R, Python, Julia, etc. Or none of the above.\nHowever, this demo uses R code to highlight advanced theme features. You’ll need to install some software if you’d like to render the demo “as-is”.\n\n\n\n\n\n\nRequired software (this demo only)\n\n\nR packages\ninstall.packages(c(\"modelsummary\", \"fixest\", \"pdftools\", \"tinytex\", \"threejs\"))\nTinyTex\nquarto install tinytex\nWhile reveal.js presentations are HTML format, we will show an example of how to embed LaTeX tables as images. This requires a working Tex distribution, of which TinyTex provides by far the easiest and lightest integration with Quarto. More details here."
  },
  {
    "objectID": "slides/test.html#components-1",
    "href": "slides/test.html#components-1",
    "title": "Introduction à l’IA",
    "section": "Components",
    "text": "Components\nOrdered and Unordered Lists\nHere we have an unordered list.\n\nfirst item\n\nsub-item\n\nsecond item\n\nAnd next we have an ordered one.\n\nfirst item\n\nsub-item\n\nsecond item"
  },
  {
    "objectID": "slides/test.html#components-2",
    "href": "slides/test.html#components-2",
    "title": "Introduction à l’IA",
    "section": "Components",
    "text": "Components\nAlerts & Cross-refs\nTo emphasize specific words or text, you can:\n\nUse the default .alert class, e.g. important note.\nUse the .fg class for custom colour, e.g. important note.\nUse the .bg class for custom background, e.g. important note.\n\nTo cross-reference, you have several options, for example:\n\nBeamer-like .button class provided by this theme, e.g. Appendix\nQuarto’s native cross-ref syntax, e.g., “See Section 6.3.”"
  },
  {
    "objectID": "slides/test.html#components-3",
    "href": "slides/test.html#components-3",
    "title": "Introduction à l’IA",
    "section": "Components",
    "text": "Components\nCitations\nCitations follow the standard Quarto format and be sourced from BibLaTex, BibTeX, or CLS files. For example:\n\nTopic 1: Spatial Frictions [@Fajgelbaum_Morales_Serrato_Zidar_2018; @Hsieh_Moretti_2019; @Moretti_2011]\nTopic 2: Blah [@Suárez_Serrato_Zidar_2016]"
  },
  {
    "objectID": "slides/test.html#components-4",
    "href": "slides/test.html#components-4",
    "title": "Introduction à l’IA",
    "section": "Components",
    "text": "Components\nBlocks\nQuarto provides dedicated environments for theorems, lemmas, and so forth.\nBut in presentation format, it’s arguably more effective just to use a Callout Block.\n\n\n\n\n\n\nRegression Specification\n\n\nThe main specification is as follows:\n\\[\ny_{it} = X_{it} \\beta + \\mu_i + \\varepsilon_{it}\n\\]"
  },
  {
    "objectID": "slides/test.html#components-5",
    "href": "slides/test.html#components-5",
    "title": "Introduction à l’IA",
    "section": "Components",
    "text": "Components\nMulticolumn I: Text only\n\n\nColumn 1\nHere is a long sentence that will wrap onto the next line as it hits the column width, and continue this way until it stops.\n\nColumn 2\nSome other text in another column.\nA second paragraph.\n\nMulticolumn support is very flexible and we can continue with a single full span column in the same slide."
  },
  {
    "objectID": "slides/test.html#components-6",
    "href": "slides/test.html#components-6",
    "title": "Introduction à l’IA",
    "section": "Components",
    "text": "Components\nMulticolumn II: Text and figures\n\n\n\n\n\nA point about the figure that is potentially important.\nAnother point about the figure that is also potentially important.\n\n\nNote that sub- and multi-panel figures are also natively supported by Quarto. See here."
  },
  {
    "objectID": "slides/test.html#components-7",
    "href": "slides/test.html#components-7",
    "title": "Introduction à l’IA",
    "section": "Components",
    "text": "Components\nMulticolumn III: Code and output\n\n\npalette(\"Classic Tableau\")\n\npar(\n  family = \"HersheySans\",\n  las = 1, pch = 19, cex = 1.5\n)\n\npairs(\n  iris[,1:4],\n  col=iris$Species\n)\n\n\n\n\n\n\n\n\nFigure 1: Pairwise scatterplot"
  },
  {
    "objectID": "slides/test.html#markdown-tables",
    "href": "slides/test.html#markdown-tables",
    "title": "Introduction à l’IA",
    "section": "Markdown tables",
    "text": "Markdown tables\nDefault table styling\nThe clean theme rolls its own minimalist aesthetic for tables. This should interface directly with Quarto’s excellent table support.\n\n\n| fruit  | price  |\n|--------|-------:|\n| apple  | 2.05   |\n| pear   | 1.37   |\n| orange | 3.09   |\n\n: Fruit prices {tbl-colwidths=\"[75,25]\"}\n\n\n\n\nFruit prices\n\n\n\n\n\n\nfruit\nprice\n\n\n\n\napple\n2.05\n\n\npear\n1.37\n\n\norange\n3.09"
  },
  {
    "objectID": "slides/test.html#regression-tables",
    "href": "slides/test.html#regression-tables",
    "title": "Introduction à l’IA",
    "section": "Regression tables",
    "text": "Regression tables\nRegression example\nThese aesthetics should carry over to any computation-based tables too.\nLet’s take a few slides to illustrate via a simple regression example:\n\nlibrary(fixest)\n\nmods = feols(\n  rating ~ complaints + privileges + learning + csw0(raises + critical),\n  data = attitude\n)\n\ndict = c(\"rating\"     = \"Overall Rating\",\n         \"complaints\" = \"Handling of Complaints\",\n         \"privileges\" = \"No Special Priviledges\",\n         \"learning\"   = \"Opportunity to Learn\",\n         \"raises\"     = \"Performance-Based Raises\",\n         \"critical\"   = \"Too Critical\")"
  },
  {
    "objectID": "slides/test.html#regression-tables-1",
    "href": "slides/test.html#regression-tables-1",
    "title": "Introduction à l’IA",
    "section": "Regression tables",
    "text": "Regression tables\nmodelsummary\nPopular regression table software should play nicely with this theme out of the box. Here’s an example using modelsummary (with the default tinytable backend1). See the next slide for the resulting table.\n\nlibrary(modelsummary) # Make sure you have &gt;=v2.0.0\n\nmodelsummary(\n  setNames(mods, c(\"(1)\", \"(2)\")),\n  coef_map = dict, stars = TRUE,\n  gof_map = NA\n  ) |&gt;\n  # some optional stylistic tweaks\n  tinytable::group_tt(j = list(\"Dep. variable: Overall Rating\" = 2:3)) |&gt;\n  tinytable::style_tt(i = 1:2, j = 2:3, background = \"pink\")\n\n\nFor extra styling options (e.g., bootstrap themes), see here."
  },
  {
    "objectID": "slides/test.html#regression-tables-1-output",
    "href": "slides/test.html#regression-tables-1-output",
    "title": "Introduction à l’IA",
    "section": "Regression tables",
    "text": "Regression tables\n\n\n\n    \n\n    \n    \n      \n        \n\n \nDep. variable: Overall Rating\n\n        \n              \n                 \n                (1)\n                (2)\n              \n        \n        + p &lt; 0.1, * p &lt; 0.05, ** p &lt; 0.01, *** p &lt; 0.001\n        \n                \n                  Handling of Complaints  \n                  0.682***\n                  0.692***\n                \n                \n                                          \n                  (0.129) \n                  (0.149) \n                \n                \n                  No Special Priviledges  \n                  -0.103  \n                  -0.104  \n                \n                \n                                          \n                  (0.129) \n                  (0.135) \n                \n                \n                  Opportunity to Learn    \n                  0.238+  \n                  0.249   \n                \n                \n                                          \n                  (0.139) \n                  (0.160) \n                \n                \n                  Performance-Based Raises\n                          \n                  -0.033  \n                \n                \n                                          \n                          \n                  (0.202) \n                \n                \n                  Too Critical            \n                          \n                  0.015   \n                \n                \n                                          \n                          \n                  (0.147)"
  },
  {
    "objectID": "slides/test.html#regression-tables-2",
    "href": "slides/test.html#regression-tables-2",
    "title": "Introduction à l’IA",
    "section": "Regression tables",
    "text": "Regression tables\nfixest::etable\nAside: We used the fantastic fixest package to estimate our regression models. fixest bundles its own powerful tabling functions. These were designed for LaTeX output, but can work with this (HTML) theme too.1\n\nSet the output: asis R chunk option in your Quarto doc.\nSet the markdown = TRUE fixest::etable option.\n\n```{{r}}\n#| output: asis\n\nsetFixest_etable(markdown = TRUE, drop = \"Constant\")\nsetFixest_dict(dict)\n\netable(mods, highlight = .(\"se\" = \"complaints\"))\n```\nDetails here. You need to install the tinytex & pdftools packages first."
  },
  {
    "objectID": "slides/test.html#regression-tables-3",
    "href": "slides/test.html#regression-tables-3",
    "title": "Introduction à l’IA",
    "section": "Regression tables",
    "text": "Regression tables\nfixest::etable (cont.)"
  },
  {
    "objectID": "slides/test.html#figure",
    "href": "slides/test.html#figure",
    "title": "Introduction à l’IA",
    "section": "Figure",
    "text": "Figure"
  },
  {
    "objectID": "slides/test.html#figure-1",
    "href": "slides/test.html#figure-1",
    "title": "Introduction à l’IA",
    "section": "Figure",
    "text": "Figure\nFull-size Figures\nYou can use the {.background-image} container environment to completely fill the slide background with an image.\nIdeally, your figure will be the same aspect ratio as the screen that you’re presenting on.\n\nThis can be a bit tricky because of the dynamic nature of reveal.js / HTML. But it’s probably something close to 16:9.\nAspect ratio can also matter for a regular full-frame images (previous slide)."
  },
  {
    "objectID": "slides/test.html#interactive-plots",
    "href": "slides/test.html#interactive-plots",
    "title": "Introduction à l’IA",
    "section": "Interactive plots",
    "text": "Interactive plots\n\n\n\n\n\n\nNote: Simple flight data example using threejs. There are many interactive plotting options beyond this. (More details.)"
  },
  {
    "objectID": "slides/test.html#other-1",
    "href": "slides/test.html#other-1",
    "title": "Introduction à l’IA",
    "section": "Other",
    "text": "Other\nWhat else can the clean theme do?\nWe have highlighted some theme-specific components in this demo.\nBut please note that all of the standard reveal.js functionality and plugins are compatible with the clean theme. This includes:\n\nchalkboard for annotating slides.\nmultiplex for enabling audience navigation of your slides.\npdf printing in case you don’t have access to a web browser.\nEtc."
  },
  {
    "objectID": "slides/test.html#summary-1",
    "href": "slides/test.html#summary-1",
    "title": "Introduction à l’IA",
    "section": "Summary",
    "text": "Summary\nA minimalist and elegant presentation theme\nThe Quarto reveal.js clean theme aims to be a minimalist and elegant presention theme. Here are some options to get you started.\nAdd the theme to an existing project.\nquarto install extension grantmcdermott/quarto-revealjs-clean\n… or, create a new project using our lean template.\nquarto use template grantmcdermott/quarto-revealjs-clean\n… or, create a new project using these demo slides as a full template.\nquarto use template grantmcdermott/quarto-revealjs-clean-demo"
  },
  {
    "objectID": "slides/test.html#references",
    "href": "slides/test.html#references",
    "title": "Introduction à l’IA",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "slides/test.html#sec-appendix",
    "href": "slides/test.html#sec-appendix",
    "title": "Introduction à l’IA",
    "section": "Appendix",
    "text": "Appendix\n\n\n\n\nTable 1: Summary of the base R attitude dataset\n\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                 \n                Unique\n                Missing Pct.\n                Mean\n                SD\n                Min\n                Median\n                Max\n                Histogram\n              \n        \n        \n        \n                \n                  rating\n                  22\n                  0\n                  64.6\n                  12.2\n                  40.0\n                  65.5\n                  85.0\n                  \n                \n                \n                  complaints\n                  23\n                  0\n                  66.6\n                  13.3\n                  37.0\n                  65.0\n                  90.0\n                  \n                \n                \n                  privileges\n                  24\n                  0\n                  53.1\n                  12.2\n                  30.0\n                  51.5\n                  83.0\n                  \n                \n                \n                  learning\n                  23\n                  0\n                  56.4\n                  11.7\n                  34.0\n                  56.5\n                  75.0\n                  \n                \n                \n                  raises\n                  21\n                  0\n                  64.6\n                  10.4\n                  43.0\n                  63.5\n                  88.0\n                  \n                \n                \n                  critical\n                  21\n                  0\n                  74.8\n                  9.9\n                  49.0\n                  77.5\n                  92.0\n                  \n                \n                \n                  advance\n                  20\n                  0\n                  42.9\n                  10.3\n                  25.0\n                  41.0\n                  72.0\n                  \n                \n        \n      \n    \n\n\n\n\n\n\nBack to main"
  },
  {
    "objectID": "posts/welcome/index.html#héberger-son-blog-avec-github-pages",
    "href": "posts/welcome/index.html#héberger-son-blog-avec-github-pages",
    "title": "Migrer son blog Wordpress vers un blog Quarto statique hébergé gratuitement avec Github Pages",
    "section": "Héberger son blog avec GitHub pages",
    "text": "Héberger son blog avec GitHub pages\n\nDéployer le blog\nAprès avoir créer un nouveau dépôt sur GitHub à partir du .git créé en local, on va dans Settings &gt; Pages puis dans Build and deployment on choisit comme source Deploy from a branch. On sélectionne la branche main et le dossier /docs puis on valide avec save. Après quelques minutes le lien vers la page du blog est généré : kevinpolisano.github.io\n\n\nNom de domaine customisé\nL’hébergeur de mon site WP était o2switch, que je recommande pour la grande réactivité de leur service technique.\nDu côté de ce registar voici les étapes à suivre :\n\nDans Espace client &gt; Commander un service choisir Commander un nom de domaine (pour ma part : kevinpolisano.fr)\nDans Espace technique &gt; Domaines configurés remplir Configurer un nom de domaine (pour ma part : kevinpolisano.fr) puis activer Let's Encrypt SSL dans l’onglet Sécurité du cPanel.\nDans Espace technique &gt; Zone Editor entrer les champs suivants (à adapter pour votre blog) :\n\n\n\n\n\nNom\nTLL\nType\nEnregistrement\n\n\n\n\nwww.kevinpolisano.fr.\n14400\nCNAME\nkevinpolisano.github.io\n\n\nkevinpolisano.fr.\n14400\nA\n185.199.108.153\n\n\nkevinpolisano.fr.\n14400\nA\n185.199.109.153\n\n\nkevinpolisano.fr.\n14400\nA\n185.199.110.153\n\n\nkevinpolisano.fr.\n14400\nA\n185.199.111.153\n\n\n\n\nJ’ai également créé un fichier CNAME à la racine du dépôt git, contenant la ligne www.kevinpolisano.fr.\n\nDu côté de GitHub Pages voici les étapes à suivre :\n\nDans Custom Domain renseigner le nouveau nom de domaine www.kevinpolisano.fr puis cliquer sur save. À ce stade j’ai obtenu à tour de rôle les messages d’erreurs suivants :\n\n\n\n\n\n\n\nImportant\n\n\n\nDNS check unsuccessful\nBoth kevinpolisano.fr and its alternate name are improperly configured Domain does not resolve to the GitHub Pages server. For more information, see documentation (NotServedByPagesError).\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDNS valid for primary\nkevinpolisano.fr is improperly configured Domain does not resolve to the GitHub Pages server. For more information, see documentation (NotServedByPagesError).\n\n\n\nJ’ai du attendre quelques heures pour que la propagation du DNS soit effective. On peut suivre celle-ci sur DNS Checker en s’assurant pour le domaine racine kevinpolisano.fr que les enregistrements A pointent vers les IP GitHub Pages; et pour le sous-domaine www.kevinpolisano.fr que l’enregistrement CNAME pointe vers kevinpolisano.github.io.\nUne fois que le bouton save donne DNS check successful on coche Enforce HTTPS et on vérifie qu’en tapant kevinpolisano.fr dans la barre de navigateur on est bien redirigé vers le blog à l’adresse https://www.kevinpolisano.fr/"
  },
  {
    "objectID": "posts/migration-wp-to-quarto/index.html",
    "href": "posts/migration-wp-to-quarto/index.html",
    "title": "Migrer son blog Wordpress vers un blog Quarto statique hébergé gratuitement avec Github Pages",
    "section": "",
    "text": "Mon site perso fait peau neuve !\nJe vous explique ici quelles étapes j’ai suivi pour effectuer la migration. Au boulot !"
  },
  {
    "objectID": "posts/migration-wp-to-quarto/index.html#exporter-ses-articles-wordpress-en-html-vers-markdown",
    "href": "posts/migration-wp-to-quarto/index.html#exporter-ses-articles-wordpress-en-html-vers-markdown",
    "title": "Migrer son blog Wordpress vers un blog Quarto statique hébergé gratuitement avec Github Pages",
    "section": "Exporter ses articles Wordpress en HTML vers Markdown",
    "text": "Exporter ses articles Wordpress en HTML vers Markdown\nJ’ai expérimenté trois solutions :\n\nGitHub - SchumacherFM/wordpress-to-hugo-exporter\nGitHub - lonekorean/wordpress-export-to-markdown\nGitHub - palaniraja/blog2md\n\nLa première conserve certaines balises HTML (typiquement pour l’usage de la couleur et la mise en forme), tandis que les deux suivantes exportent en pur Markdown.\n\nWordpress to Hugo Exporter\nLa première solution consiste à uploader l’archive sur le site WP dans le dossier wp-content/plugins puis d’activer le plugin et utiliser Outils &gt; Export Hugo. Ce dernier n’a pas fonctionné donc j’ai utilisé le Terminal de mon serveur d’hébergement (O2switch) (comme expliqué ici) :\ncd wp-content/plugins/wordpress-to-hugo-exporter/\nphp hugo-export-cli.php\nLe script créé un fichier /tmp/wp-hugo.zip (cela peut prendre quelques minutes). Dans le gestionnaire de fichiers (via le Cpanel d’O2switch) j’ai effectué une recherche du fichier, il m’indique que celui-ci se trouve dans le dossier caché .cagefs/tmp/.\n\n\nWordpress export to Markdown\nPremièrement on effectue un export du contenu complet de WP au format export.xml, que l’on place dans l’archive du code téléchargé. Puis on exécute le script :\nnpm install && node index.js\n\n\nBlog2md\nDe même on exécute :\nnpm install && node index.js w export.xml out"
  },
  {
    "objectID": "posts/migration-wp-to-quarto/index.html#wordpress-to-hugo-exporter",
    "href": "posts/migration-wp-to-quarto/index.html#wordpress-to-hugo-exporter",
    "title": "Migrer son blog Wordpress vers un blog Quarto statique hébergé gratuitement avec Github Pages",
    "section": "Wordpress to Hugo Exporter",
    "text": "Wordpress to Hugo Exporter\nLa première solution consiste à uploader l’archive sur le site WP dans le dossier wp-content/plugins puis d’activer le plugin et utiliser Outils &gt; Export Hugo. Ce dernier n’a pas fonctionné donc j’ai utilisé le Terminal de mon serveur d’hébergement (O2switch) (comme expliqué ici) :\ncd wp-content/plugins/wordpress-to-hugo-exporter/\nphp hugo-export-cli.php\nLe script créé un fichier /tmp/wp-hugo.zip (cela peut prendre quelques minutes). Dans le gestionnaire de fichiers (via le Cpanel d’O2switch) j’ai effectué une recherche du fichier, il m’indique que celui-ci se trouve dans le dossier caché .cagefs/tmp/.\n\nWordpress export to Markdown\nPremièrement on effectue un export du contenu complet de WP au format export.xml, que l’on place dans l’archive du code téléchargé. Puis on exécute le script :\nnpm install && node index.js\n\n\nBlog2md\nDe même on exécute :\nnpm install && node index.js w export.xml out"
  },
  {
    "objectID": "posts/migration-wp-to-quarto/index.html#nettoyage-des-articles-exportés-liens-images",
    "href": "posts/migration-wp-to-quarto/index.html#nettoyage-des-articles-exportés-liens-images",
    "title": "Migrer son blog Wordpress vers un blog Quarto statique hébergé gratuitement avec Github Pages",
    "section": "Nettoyage des articles exportés, liens, images, …",
    "text": "Nettoyage des articles exportés, liens, images, …"
  },
  {
    "objectID": "posts/migration-wp-to-quarto/index.html#créer-son-blog-avec-quarto",
    "href": "posts/migration-wp-to-quarto/index.html#créer-son-blog-avec-quarto",
    "title": "Migrer son blog Wordpress vers un blog Quarto statique hébergé gratuitement avec Github Pages",
    "section": "Créer son blog avec Quarto",
    "text": "Créer son blog avec Quarto\n\nOuvrir RStudio\nCréer un nouveau projet Create Quarto Blog\nCocher Create a git repository\n\nUn blog Quarto vierge est ainsi créé contenant ces fichiers :\n\n_quarto.yml : Fichier du projet Quarto\nindex.qmd : Page d’accueil\nabout.qmd : Page “À propos”\nposts/ : Répertoire contenant les posts en Quarto Markdown (.qmd)\nposts/_metadata.yml : Options partagées des posts\nstyles.css : CSS customisé pour le style du blog\nMyBlog.Rproj : Raccourci d’ouverture du projet Quarto\n\nDans le fichier _quarto.yml ajoutez la ligne output-dir pour renseigner le répertoire cible pour la génération du site :\nproject:\n  type: website\n  output-dir: docs\nEnfin pour le générer et avoir le rendu du blog il suffit d’exécuter build &gt; Render Website"
  },
  {
    "objectID": "posts/migration-wp-to-quarto/index.html#héberger-son-blog-avec-github-pages",
    "href": "posts/migration-wp-to-quarto/index.html#héberger-son-blog-avec-github-pages",
    "title": "Migrer son blog Wordpress vers un blog Quarto statique hébergé gratuitement avec Github Pages",
    "section": "Héberger son blog avec GitHub pages",
    "text": "Héberger son blog avec GitHub pages\n\nDéployer le blog\nAprès avoir créer un nouveau dépôt sur GitHub à partir du .git créé en local, on va dans Settings &gt; Pages puis dans Build and deployment on choisit comme source Deploy from a branch. On sélectionne la branche main et le dossier /docs puis on valide avec save. Après quelques minutes le lien vers la page du blog est généré : kevinpolisano.github.io\n\n\nNom de domaine customisé\nL’hébergeur de mon site WP était o2switch, que je recommande pour la grande réactivité de leur service technique.\nDu côté de ce registar voici les étapes à suivre :\n\nDans Espace client &gt; Commander un service choisir Commander un nom de domaine (pour ma part : kevinpolisano.fr)\nDans Espace technique &gt; Domaines configurés remplir Configurer un nom de domaine (pour ma part : kevinpolisano.fr) puis activer Let's Encrypt SSL dans l’onglet Sécurité du cPanel.\nDans Espace technique &gt; Zone Editor entrer les champs suivants (à adapter pour votre blog) :\n\n\n\n\n\nNom\nTLL\nType\nEnregistrement\n\n\n\n\nwww.kevinpolisano.fr.\n14400\nCNAME\nkevinpolisano.github.io\n\n\nkevinpolisano.fr.\n14400\nA\n185.199.108.153\n\n\nkevinpolisano.fr.\n14400\nA\n185.199.109.153\n\n\nkevinpolisano.fr.\n14400\nA\n185.199.110.153\n\n\nkevinpolisano.fr.\n14400\nA\n185.199.111.153\n\n\n\n\nJ’ai également créé un fichier CNAME à la racine du dépôt git, contenant la ligne www.kevinpolisano.fr.\n\nDu côté de GitHub Pages voici les étapes à suivre :\n\nDans Custom Domain renseigner le nouveau nom de domaine www.kevinpolisano.fr puis cliquer sur save. À ce stade j’ai obtenu à tour de rôle les messages d’erreurs suivants :\n\n\n\n\n\n\n\nImportant\n\n\n\nDNS check unsuccessful\nBoth kevinpolisano.fr and its alternate name are improperly configured Domain does not resolve to the GitHub Pages server. For more information, see documentation (NotServedByPagesError).\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDNS valid for primary\nkevinpolisano.fr is improperly configured Domain does not resolve to the GitHub Pages server. For more information, see documentation (NotServedByPagesError).\n\n\n\nJ’ai du attendre quelques heures pour que la propagation du DNS soit effective. On peut suivre celle-ci sur DNS Checker en s’assurant pour le domaine racine kevinpolisano.fr que les enregistrements A pointent vers les IP GitHub Pages; et pour le sous-domaine www.kevinpolisano.fr que l’enregistrement CNAME pointe vers kevinpolisano.github.io.\nUne fois que le bouton save donne DNS check successful on coche Enforce HTTPS et on vérifie qu’en tapant kevinpolisano.fr dans la barre de navigateur on est bien redirigé vers le blog à l’adresse https://www.kevinpolisano.fr/"
  },
  {
    "objectID": "about.html#où-suis-je",
    "href": "about.html#où-suis-je",
    "title": "Kévin Polisano",
    "section": "Où suis-je ?",
    "text": "Où suis-je ?\n\n\nRéseaux sociaux \n\n Mastodon\n Twitter\n Instagram\n YouTube\n Facebook\n\n\nAcadémique\n\n Page professionnelle\n Linkedin\n Google Scholar\n Researchgate\n Publons\n Academia\n HAL\n Github\n Gitlab\n\n\nLoisirs\n\n Musique\n Lectures\n Échecs"
  },
  {
    "objectID": "about.html#dans-mes-loisirs",
    "href": "about.html#dans-mes-loisirs",
    "title": "Kévin Polisano",
    "section": "Dans mes loisirs :",
    "text": "Dans mes loisirs :"
  },
  {
    "objectID": "about.html#où-me-trouver",
    "href": "about.html#où-me-trouver",
    "title": "Kévin Polisano",
    "section": "Où me trouver ?",
    "text": "Où me trouver ?\n\n\nRéseaux sociaux \n\n Mastodon\n Twitter\n Instagram\n YouTube\n Facebook\n\n\nAcadémique\n\n Page pro\n Linkedin\n Google Scholar\n Researchgate\n Publons\n Academia\n HAL\n Github\n Gitlab\n\n\nLoisirs\n\n Musique\n Lectures\n Échecs"
  }
]